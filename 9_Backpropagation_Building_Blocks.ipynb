{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks of Backpropgation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRADINETS OF THE LOSS WITH RESPECT TO WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.  14.  14. ]\n",
      " [21.  21.  21. ]\n",
      " [10.  10.  10. ]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "\n",
    "dvalues = np.array([[1.,1.,1.],[2.,2.,2.],[3.,3.,3.]])\n",
    "\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[1,2,3,2.5],[2.,5.,-1.,2],[3.,3.,3.,-0.8]])\n",
    "# sum weights of given input \n",
    "# and multiply by the passed-in gradient for this neuron\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "print(dweights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRADINETS OF THE LOSS WITH RESPECT TO BIASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "dvalues = np.array([[1.,1.,1.],[2.,2.,2.],[3.,3.,3.]])\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1,neurons)\n",
    "biases = np.array([[2,3,0.5]])\n",
    "# dbiases - sum values , do this over samples (first axis), keepdims\n",
    "# since this by default will produce a plain list -\n",
    "\n",
    "dbiases = np.sum(dvalues,axis=0,keepdims=True)\n",
    "print(dbiases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRADINETS OF THE LOSS WITH RESPECT TO INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "dvalues = np.array([[1.,1.,1.],[2.,2.,2.],[3.,3.,3.]])\n",
    "\n",
    "# we have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs , thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[0.2,0.8,-0.5,1],[0.5,-0.91,0.26,-0.5],[-0.26,-0.27,0.17,0.87]]).T\n",
    "\n",
    "# sum weights of given input\n",
    "# and multiply by the passed in gradient for this neuron\n",
    "dinputs = np.dot(dvalues,weights.T)\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDING THE \"BACKWARD\" METHOD IN THE LAYER-DENSE CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
