{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Classifier - Combined Softmax activation and cross-entropy loss for faster backward step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n",
      "[[0.35667494 2.30258509 1.60943791]\n",
      " [2.30258509 0.69314718 0.91629073]\n",
      " [3.91202301 0.10536052 2.52572864]]\n",
      "0.38506088005216804\n",
      "[0.35667494 0.69314718 0.10536052]\n",
      "0.38506088005216804\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "%run 5_Cross_Entropy_Loss.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    " # Forward pass\n",
    " def forward(self, inputs):\n",
    " # Get unnormalized probabilities\n",
    "  exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    " # Normalize them for each sample\n",
    "  probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "  self.output = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross- entropy loss for faster backward step\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    # Create activation and loss function objects\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "\n",
    "    def forward(self,inputs,y_true):\n",
    "        # Output layer's actiavtion function\n",
    "\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output \n",
    "\n",
    "        self.output = self.activation.output \n",
    "        # calculate and return loss value \n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "\n",
    "        # Backward pass\n",
    "\n",
    "    def backward(self,dvalues,y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "\n",
    "        # Copy so we can safely modify \n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Calculate gradient\n",
    "\n",
    "        self.dinputs[range(samples),y_true] -= 1\n",
    "\n",
    "        # Normalize gradient\n",
    "\n",
    "        self.dinputs = self.dinputs / samples \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7 , 0.1 , 0.2 ],\n",
       "       [0.1 , 0.5 , 0.4 ],\n",
       "       [0.02, 0.9 , 0.08]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " softmax_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients: combined loss and activation :\n",
      "[[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n"
     ]
    }
   ],
   "source": [
    "class_target = np.array([0,1,1])\n",
    "softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "softmax_loss.backward(softmax_outputs, class_target)\n",
    "dvalues1 = softmax_loss.dinputs \n",
    "print('Gradients: combined loss and activation :')\n",
    "print(dvalues1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
